{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "mount_file_id": "1lmfo7ssKqbKptISKbXeLYLvhAo_4Exvl",
      "authorship_tag": "ABX9TyPbS3ySufdE2rB+lN9Uh00p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khakhiD/ML/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VtZnrkEUzoxu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "import torch\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_box(obj):\n",
        "    xmin = int(obj.find('xmin').text)\n",
        "    ymin = int(obj.find('ymin').text)\n",
        "    xmax = int(obj.find('xmax').text)\n",
        "    ymax = int(obj.find('ymax').text)\n",
        "    return [xmin, ymin, xmax, ymax]\n",
        "\n",
        "def generate_label(obj):\n",
        "    if obj.find('name').text == 'with_mask':\n",
        "        return 1\n",
        "    elif obj.find('name').text == 'mask_weared_incorrect':\n",
        "        return 2\n",
        "    return 0\n",
        "\n",
        "def generate_target(image_id, file):\n",
        "    with open(file) as f:\n",
        "        data = f.read()\n",
        "        soup = BeautifulSoup(data, 'xml')\n",
        "        objects = soup.find_all('object')\n",
        "        \n",
        "        num_objs = len(objects)\n",
        "        \n",
        "        # Bounding boxes for objects\n",
        "        # In coco format, bbox = [xmin, ymin, width, height]\n",
        "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for i in objects:\n",
        "            boxes.append(generate_box(i))\n",
        "            labels.append(generate_label(i))\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        \n",
        "        # Labels (In my case, I only one class: target class or background)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "        \n",
        "        # Tensorise img_id\n",
        "        img_id = torch.tensor([image_id])\n",
        "        \n",
        "        # Annotation is in dictionary format\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = img_id\n",
        "        \n",
        "        return target"
      ],
      "metadata": {
        "id": "ceFNebOiAjpZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "pyuKjPgyD6rx",
        "outputId": "44070812-aec3-4282-b113-568d523b035b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b7e31eba-2fd1-40a9-b5ac-3671b42f81f7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b7e31eba-2fd1-40a9-b5ac-3671b42f81f7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"khakhid\",\"key\":\"47c5de2314f1541c4b46bf27f934afad\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -1ha kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmrz9syaEF2Z",
        "outputId": "dae71d9e-9929-4fb4-a4e3-2753accffb6d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Permission Warning 방지\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "JP_iPQqEEUv3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andrewmvd/face-mask-detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdHNBfp4EfAY",
        "outputId": "d1f87623-1717-4d17-965d-7f31806ae028"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "face-mask-detection.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBT9xabbErBb",
        "outputId": "0e850511-6818-4f8c-a602-1de09ac9b4d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " drive\t\t\t  'kaggle (1).json'   kaggle.json\n",
            " face-mask-detection.zip  'kaggle (2).json'   sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/face-mask-detection.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrS7IIToFInf",
        "outputId": "ad50c3e8-a1c3-42f0-d664-e29db3a9cea2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/face-mask-detection.zip\n",
            "replace annotations/maksssksksss0.xml? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = list(sorted(os.listdir(\"/content/drive/MyDrive/Colab Notebooks/face-mask-detection/images/\")))\n",
        "labels = list(sorted(os.listdir(\"/content/drive/MyDrive/Colab Notebooks/face-mask-detection/annotations/\")))"
      ],
      "metadata": {
        "id": "RHZwFtHsFULX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskDataset(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them too 이미지 파일 다 불러와서 정렬\n",
        "        # ensure that they are aligned 정렬됐는지 확인\n",
        "        self.imgs = list(sorted(os.listdir(\"/content/drive/MyDrive/Colab Notebooks/face-mask-detection/images/\")))\n",
        "#         self.labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        file_image = 'maksssksksss'+ str(idx) + '.png'\n",
        "        file_label = 'maksssksksss'+ str(idx) + '.xml'\n",
        "        img_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/face-mask-detection/images/\", file_image)\n",
        "        label_path = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/face-mask-detection/annotations/\", file_label)\n",
        "        img = Image.open(img_path).convert(\"RGB\") #BGR 사진을 RGB로 변경하기\n",
        "        #Generate Label        라벨 생성\n",
        "        target = generate_target(idx, label_path)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "MczO9pJqF63y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ]) #ToTensor() - 데이터를 tensor로 바꿔준다. https://wikidocs.net/157285"
      ],
      "metadata": {
        "id": "S8BNh2OrGHt2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "dataset = MaskDataset(data_transform)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "u543V20GGISV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0Cre--QH49r",
        "outputId": "3ef56af7-a5c5-4993-95c5-81834172c1eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "E4fiKzaAH9Jx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading:\n",
        "#\"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\"\n",
        "# to /root/.cache/torch/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
        "model = get_model_instance_segmentation(3)"
      ],
      "metadata": {
        "id": "AIhtmopQIi9_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "for imgs, annotations in data_loader:\n",
        "    imgs = list(img.to(device) for img in imgs)\n",
        "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "    print(annotations)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1aPV1HJGn96",
        "outputId": "3509fd50-1643-48b6-eee2-325e3227a75a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'boxes': tensor([[ 79., 105., 109., 142.],\n",
            "        [185., 100., 226., 144.],\n",
            "        [325.,  90., 360., 141.]], device='cuda:0'), 'labels': tensor([0, 1, 0], device='cuda:0'), 'image_id': tensor([0], device='cuda:0')}, {'boxes': tensor([[321.,  34., 354.,  69.],\n",
            "        [224.,  38., 261.,  73.],\n",
            "        [299.,  58., 315.,  81.],\n",
            "        [143.,  74., 174., 115.],\n",
            "        [ 74.,  69.,  95.,  99.],\n",
            "        [191.,  67., 221.,  93.],\n",
            "        [ 21.,  73.,  44.,  93.],\n",
            "        [369.,  70., 398.,  99.],\n",
            "        [ 83.,  56., 111.,  89.]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'image_id': tensor([1], device='cuda:0')}, {'boxes': tensor([[ 68.,  42., 105.,  69.],\n",
            "        [154.,  47., 178.,  74.],\n",
            "        [238.,  34., 262.,  69.],\n",
            "        [333.,  31., 366.,  65.]], device='cuda:0'), 'labels': tensor([1, 1, 1, 2], device='cuda:0'), 'image_id': tensor([2], device='cuda:0')}, {'boxes': tensor([[ 52.,  53.,  73.,  76.],\n",
            "        [ 72.,  53.,  92.,  75.],\n",
            "        [112.,  51., 120.,  68.],\n",
            "        [155.,  60., 177.,  83.],\n",
            "        [189.,  59., 210.,  80.],\n",
            "        [235.,  57., 257.,  78.],\n",
            "        [289.,  60., 309.,  83.],\n",
            "        [313.,  68., 333.,  90.],\n",
            "        [351.,  35., 364.,  59.]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), 'image_id': tensor([3], device='cuda:0')}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "num_epochs = 25\n",
        "model.to(device)\n",
        "    \n",
        "# parameters\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "len_dataloader = len(data_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    i = 0    \n",
        "    epoch_loss = 0\n",
        "    for imgs, annotations in data_loader:\n",
        "        i += 1\n",
        "        imgs = list(img.to(device) for img in imgs)\n",
        "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "        loss_dict = model([imgs[0]], [annotations[0]])\n",
        "        losses = sum(loss for loss in loss_dict.values())        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step() \n",
        "#         print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
        "        epoch_loss += losses\n",
        "    print(epoch_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hgOmZ7kI74m",
        "outputId": "4d4c4752-c2f2-43db-a672-b27cb74e8499"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(86.1549, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(61.5217, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(48.3184, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(41.7371, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(41.8040, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(37.8991, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(36.1436, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(30.5637, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(29.3548, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(29.0730, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(26.2626, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(25.6681, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(30.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(26.3212, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(22.9832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(23.3192, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(24.6303, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(22.7668, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(22.9328, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(22.3752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(21.7908, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(20.1885, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(20.5858, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(20.8600, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(19.0823, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for imgs, annotations in data_loader:\n",
        "        imgs = list(img.to(device) for img in imgs)\n",
        "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
        "        break"
      ],
      "metadata": {
        "id": "EQnG9GX9Q52C"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "preds = model(imgs)\n",
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yYrxMltQ6xc",
        "outputId": "4aba40d7-b47e-408a-e0e1-6f4c09c9a088"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'boxes': tensor([[184.3963, 100.5598, 226.8540, 144.3602],\n",
              "          [321.1229,  92.9683, 359.1706, 141.9732]], device='cuda:0',\n",
              "         grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1, 2], device='cuda:0'),\n",
              "  'scores': tensor([0.9973, 0.0568], device='cuda:0', grad_fn=<IndexBackward0>)},\n",
              " {'boxes': tensor([[ 20.8375,  72.8093,  42.1878,  94.0443],\n",
              "          [224.7705,  35.9579, 257.9628,  76.3380],\n",
              "          [319.4587,  33.2142, 351.5923,  71.6000],\n",
              "          [143.5195,  72.9361, 172.4517, 114.4918],\n",
              "          [298.7391,  59.0792, 312.8836,  80.3377],\n",
              "          [192.0907,  66.7457, 214.9459,  94.5498],\n",
              "          [367.4879,  70.6762, 392.1494,  97.6532],\n",
              "          [367.8783,  71.2153, 389.5656, 100.5559],\n",
              "          [ 78.3219,  55.7528, 110.3322,  87.1204],\n",
              "          [144.0061,  73.1024, 171.9035, 109.7769]], device='cuda:0',\n",
              "         grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1, 1, 1, 1, 1, 1, 2, 1, 2, 2], device='cuda:0'),\n",
              "  'scores': tensor([0.9963, 0.9935, 0.9917, 0.9839, 0.9835, 0.9664, 0.6729, 0.5464, 0.1754,\n",
              "          0.1586], device='cuda:0', grad_fn=<IndexBackward0>)},\n",
              " {'boxes': tensor([[ 71.5813,  43.6372, 100.2241,  70.3692],\n",
              "          [155.9254,  49.5940, 175.1065,  74.8335],\n",
              "          [239.9569,  33.6630, 259.4317,  71.4864],\n",
              "          [331.0126,  29.8442, 366.7687,  68.1569],\n",
              "          [329.0603,  31.3305, 366.6802,  65.6094],\n",
              "          [219.3011, 241.9491, 237.4701, 277.2685],\n",
              "          [237.7026,  33.5711, 260.5442,  70.2605]], device='cuda:0',\n",
              "         grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1, 1, 1, 1, 2, 1, 2], device='cuda:0'),\n",
              "  'scores': tensor([0.9934, 0.9907, 0.9614, 0.8824, 0.5948, 0.1679, 0.1012],\n",
              "         device='cuda:0', grad_fn=<IndexBackward0>)},\n",
              " {'boxes': tensor([[235.6324,  55.8220, 256.4480,  78.0290],\n",
              "          [154.9024,  58.7163, 176.7330,  82.1806],\n",
              "          [ 71.3377,  50.9277,  90.3784,  75.1093],\n",
              "          [ 51.3855,  52.6821,  72.6831,  76.4197],\n",
              "          [313.6017,  67.2900, 332.2164,  89.5584],\n",
              "          [286.8056,  60.3219, 309.8157,  81.3370],\n",
              "          [186.1001,  57.3854, 211.3936,  80.0029],\n",
              "          [349.7347,  34.5506, 362.4123,  56.9284],\n",
              "          [275.2714,  77.5883, 285.7721,  95.4157],\n",
              "          [121.4724,  56.0122, 147.7381,  90.6857],\n",
              "          [349.6938,  34.7358, 364.5140,  56.4257],\n",
              "          [ 51.5013,  54.4237,  81.9117,  75.3949]], device='cuda:0',\n",
              "         grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2], device='cuda:0'),\n",
              "  'scores': tensor([0.9956, 0.9956, 0.9952, 0.9947, 0.9934, 0.9933, 0.9793, 0.7630, 0.5033,\n",
              "          0.2365, 0.1651, 0.0533], device='cuda:0', grad_fn=<IndexBackward0>)}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to plot Image"
      ],
      "metadata": {
        "id": "x42ZuYcaQ-hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(img_tensor, annotation, block=True):\n",
        "    \n",
        "    fig,ax = plt.subplots(1)\n",
        "    img = img_tensor.detach().cpu().numpy()\n",
        "    img = np.transpose(img, (1,2,0))\n",
        "    img = img.astype(np.uint8).copy()\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(img)\n",
        "    \n",
        "    for box in annotation[\"boxes\"]:\n",
        "        xmin, ymin, xmax, ymax = box\n",
        "\n",
        "        rect = patches.Rectangle((xmin,ymin), (xmax-xmin), (ymax-ymin), linewidth=1)\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "u_cZK7NVQ9W3"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prediction\")\n",
        "plot_image(imgs[2], preds[2])\n",
        "print(\"Target\")\n",
        "plot_image(imgs[2], annotations[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "TwGHThC7RI9K",
        "outputId": "b817a2d1-3ad0-4cbd-cc74-e7fcc5782fb1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-4e7698ebe7f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-7598a9fd1f73>\u001b[0m in \u001b[0;36mplot_image\u001b[0;34m(img_tensor, annotation, block)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxmax\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mymax\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Add the patch to the Axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_patch\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1919\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_patch_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1920\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_patch_limits\u001b[0;34m(self, patch)\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0mvertices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1939\u001b[0;31m             \u001b[0mxys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1940\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m                 patch_to_data = (patch.get_data_transform() -\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mget_patch_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rect_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36m_update_patch_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \"\"\"\n\u001b[1;32m    755\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_extents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mrot_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mrot_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate_deg_around\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mfrom_extents\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0maxis\u001b[0m \u001b[0mincreases\u001b[0m \u001b[0mupwards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \"\"\"\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tjnGwnBlRK9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
